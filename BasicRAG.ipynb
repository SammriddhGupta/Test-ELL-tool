{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb254acb-3ab5-40e4-b47f-1cc9f2baff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bee8680-430e-49c0-8c91-fa9f1257e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5d033-747f-4364-a970-fe3683bcb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac685742-cc45-45f6-94c1-77d0211df73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb30e55-5873-4607-b0b7-2ce88817f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59f03aa-a28a-4586-95ee-13173a38e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USER_AGENT'] = 'DefaultLangchainUserAgent'\n",
    "# https://github.com/langchain-ai/rag-from-scratch/issues/24\n",
    "# https://api.python.langchain.com/en/latest/_modules/langchain_community/utils/user_agent.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6dac844-69f8-49e3-9dfe-f8a885b814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce9ec1e-4d36-4419-bd63-0645781614c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8c966c-5278-436d-8735-9b8dac116ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de2235de-b2e3-4d5d-9f6f-cf69d477ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs) # can use this command to see how the entire webpage has been condensed to a huge text block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4234de7-bff0-4fd9-9b1d-325e820a0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e757dc-ff71-4c27-9d0b-054e57aae49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f57a7937-db1f-4312-8a67-fc683176addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35feb1d5-a252-404e-9c39-d10994667150",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6212a34-ad32-405a-8892-46d1ee1e3340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique that breaks down complex tasks into smaller and simpler steps to enhance model performance. It involves transforming big tasks into multiple manageable tasks using methods like Chain of Thought or Tree of Thoughts. Task decomposition can be done through simple prompting, task-specific instructions, or with human inputs.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0efb4995-2c66-400c-9bee-e526b60381da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The types of memory include Short-Term Memory (STM) or Working Memory, Long-Term Memory (LTM) with subtypes Explicit/Declarative and Implicit/Procedural memory, and Sensory Memory which retains sensory information briefly after stimuli end. Each type of memory serves different functions and durations in the human brain.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What are types of memory?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9b5c09b-1226-4ea6-8c30-1d5fa86997fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MIPS stands for Maximum Inner Product Search, which involves saving embedding representations of information into a vector store database to support fast maximum inner-product search. Commonly used with approximate nearest neighbors algorithms to optimize retrieval speed. It is a tool that can extend the capabilities of models by alleviating the restriction of finite attention span.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Whats mips?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adb826ec-0df9-4b8f-ad1a-fd28feaad471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAISS is a similarity search system developed by Facebook AI. It applies vector quantization to partition the vector space into clusters for efficient search. The search process involves looking for cluster candidates with coarse quantization followed by finer quantization within clusters.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what's FAISS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a72082c6-e187-414e-a1f4-4e0f80627e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chain of Hindsight (CoH) is a framework that presents a model with a sequence of past outputs, each annotated with feedback, to improve its own outputs over time. The model is fine-tuned to predict the final output based on a sequence of feedback tuples ranked by reward. CoH aims to train the model to self-reflect and produce better outputs by learning from a history of sequentially improved outputs.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is chain of hindsight?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea3abf0-6018-4e85-8bc1-e59cfc721143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LangChainEnv] *",
   "language": "python",
   "name": "conda-env-LangChainEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
